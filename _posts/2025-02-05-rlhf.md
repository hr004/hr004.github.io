---
title: Reinforcement Learning From Human Feedback
author: Hafizur Rahman
date: 2025-01-04 22:33:00 -0500 
categories: [Reinforcement Learning]
tags: [LLMs, Scaling Laws]
math: true
# image: /assets/img/sample/devices-mockup.png
---

<!-- 
## Titles

---

# H1

<h2 data-toc-skip>H2</h2>

<h3 data-toc-skip>H3</h3>

<h4>H4</h4>

--- -->

### Deep Reinforcement Leanring From Human Preferences

From neural scalling laws we know that training larger model gives the model ability to solve novel problems. But training bigger models does not inherently mean the model will follow user’s intent. So how can we align our preferences with the models output?  One way to do it is Bradley Terry Model. It has similarity with logistic regression. In logistic regression, we know the parameters and then infer the functional form of the $Pr(A≻B)$ but in Bradley terry model we do the opposite where we know the rank and want to infer the parameters.

##### The Bradley-Terry Model: Understanding Pairwise Comparisons  

The Bradley-Terry model is used to analyze pairwise comparisons between items based on their latent strengths. Given two items, \( A \) and \( B \), each with strengths \( \pi_A \) and \( \pi_B \), the probability that \( A \) wins over \( B \) is:  

$$
P(A \succ B) = \frac{\pi_A}{\pi_A + \pi_B}
$$  

where \( \pi_A \) and \( \pi_B \) are positive parameters representing their relative strengths.  

##### Generalization to Multiple Items  

For a set of \( n \) items, if item \( i \) has won \( y_{ij} \) times against item \( j \), the probability of \( i \) winning is:  

$$
P(i \succ j) = \frac{\pi_i}{\pi_i + \pi_j}, \quad \log \pi_i = \theta_i
$$  

Since \( \pi_i \) is always positive, we set \( \pi_i = e^{\theta_i} \), transforming the probability function into a logistic form:  

$$
P(i \succ j) = \frac{e^{\theta_i}}{e^{\theta_i} + e^{\theta_j}} = \frac{1}{1 + e^{-(\theta_i - \theta_j)}}
$$  

which resembles the logistic function commonly used in statistics and machine learning.  

##### Maximum Likelihood Estimation  

To estimate the strengths \( \theta_i \), we maximize the likelihood function:  

$$
L(\theta) = \prod_{i,j} P(i \succ j)^{y_{ij}} (1 - P(i \succ j))^{y_{ji}}
$$  

Taking the logarithm gives:  

$$
\log L(\theta) = \sum_{i,j} y_{ij} \log P(i \succ j) + y_{ji} \log (1 - P(i \succ j))
$$  

Substituting \( P(i \succ j) \), we obtain an iterative update rule for \( \theta_i \):  

$$
\theta_i^{(t+1)} = \theta_i^{(t)} + \eta \sum_{j \neq i} \left[ y_{ij} - (y_{ij} + y_{ji}) P(i \succ j) \right]
$$  

where \( \eta \) is the learning rate. This iterative approach refines the strength estimates until convergence.  


The Bradley-Terry model is widely applied in ranking systems, such as the **Elo rating system** used in chess and online gaming. It is also useful in **preference learning**, where recommender systems infer user preferences from pairwise comparisons. Due to its simplicity and interpretability, this model remains a fundamental tool in ranking and decision-making tasks.  




### References